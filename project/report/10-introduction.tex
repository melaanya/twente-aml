\section{Introduction}
\label{sec:introduction}

Gradient boosting is an ensemble machine learning algorithm that generates a strong prediction model based on several weaker models such as decision trees. Gradient boosted decision trees are built sequentially where the newest tree corrects the errors made by the last tree, making the model becomes more expressive with addition of each new tree. The algorithm seeks to minimize a loss function using Gradient Descent so that the sum of residuals is close to zero, resulting in accurate predictions. This ensemble technique has proven to be very useful in machine learning, and as such, has been used widely in various machine learning competitions such as the ones hosted by Kaggle. Some of the widely used implementations of this algorithm are XGBoost, sklearn, LightGBM etc. XGBoost in particular has been key to a lot of winning models in Kaggle competitions dues to it's speed and performance. A newer implementation of this algorithm is CatBoost, created by the Russian company Yandex.

CatBoost is a recently released open-source gradient boosting library with categorical feature support. According to Yandex, CatBoost offers fast inference, categorical feature support, and improved accuracy among other interesting features. In their paper ‘Fighting biases with dynamic boosting’ [\textbf{\textit{reference}}], Yandex has shown that CatBoost outperforms a number of current gradient boosting implementations including XGBoost. This project aims to compare the performances of three gradient boosting libraries, viz. XGBoost, sklearn, and CatBoost on the regression task of predicting housing prices. 
 
Housing market prediction problems are often used as the ‘Hello World’ of Machine Learning to teach beginners about Supervised Learning and Regression. Apart from being easy to understand, housing market prediction is also a real-world application of machine learning as accurately predicting housing prices can have a meaningful impact on the real estate markets, the stock markets, economy, as well as sellers and buyers. For the scope of this project, three datasets were chosen: Boston Housing[\textbf{\textit{reference}}], Sberbank Russian Housing Market[\textbf{\textit{reference}}], and the Ames Housing dataset[\textbf{\textit{reference}}], each with vastly different characteristics.

The main focus of this project is the performance comparisons of these three libraries on the datasets above, with special attention given to how each of the libraries handle the categorical attributes, training time taken, performance of each model with default hyperparameters(such as learning rate, max. depth of tree, etc.) vs tuned, performance on different datasets(such as large vs small, wide vs thin, etc.). It is also worthwhile to note that while CatBoost offers fast training on GPU, due to certain limitations, this project only compares these libraries on their CPU versions.

In upcoming sections, this paper will focus on some of the related work on Gradient Boosted Decision Trees(GBDT), the datasets used in this project, the methodology followed in order to make the comparisons, the results observed, a discussion of the results and a conclusion with ideas on potential future work.