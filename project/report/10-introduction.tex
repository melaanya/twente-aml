\section{Introduction}
\label{sec:introduction}

Gradient boosting is an ensemble machine learning algorithm that results in a strong prediction model based on several weak estimators e.g. decision trees. Gradient boosted decision trees (GBDT) are built sequentially so as each new tree attempts to correct the errors made by the predecessors and cover the discrepancy between the target function and the current prediction. This ensemble technique has been both theoretically and empirically proven to be advantageous in various machine learning problems and is widely employed in various machine learning competitions. There are several widely used implementations of this algorithm such as XGBoost and GradientBoosting from sklearn. XGBoost in particular has been a key component to a lot of winning models in Kaggle competitions due to its speed and performance. A recently appeared implementation of this algorithm is created by the Russian company Yandex and is called CatBoost due to its special method of categorical features utilization.

CatBoost is an open-source gradient boosting library which, according to Yandex announcements, offers fast inference, categorical feature support, and thus, improved prediction effectiveness. It is claimed that CatBoost outperforms existing implementations of GDBT in classification tasks on multiple benchmarks \cite{catboost2017yandex} whereas there is no information about its results in regression problems. The main goal of this project is to assess the performance of CatBoost in the regression task of housing price prediction on multiple datasets and compare it with the performance of two other state-of-the art GBDT libraries, namely XGBoost and sklearn.

Apart from being a real-world application of GDBT, housing price prediction is a good example of regression problem in which the model has to output a continuous variable. In the scope of this project we make use of three datasets: Boston Housing \cite{boston1978housing}, the Ames Housing \cite{de2011ames} and Sberbank Russian Housing Market \cite{sberbank2017housing} datasets, each with vastly different characteristics.

In this paper we discuss the key improvements for GDBT implemented in CatBoost and train CatBoostRegressor together with XGBoostRegressor from XGBoost and GradientBoostingRegressor from sklearn. We evaluate their performance on three datasets for housing price prediction with different characteristics and discover that there is no clear winner among these three models. Even though CatBoost succeeds to outperform others on Boston dataset, there is no clear tendency overall. Thereafter, we perform some analysis of categorical feature importance and discuss several options for future work.

The remainder of the paper is structured as follows. We discuss the related work in \cref{sec:related-work}. \cref{sec:data} describes the data used for training and evaluation. \cref{sec:methodology} addresses the methodology of our research. In \cref{sec:results}, we present experimental results whereas in \cref{sec:discussion} we reflect on the performed investigation. We conclude this research and present some ideas for future work in \cref{sec:results}.