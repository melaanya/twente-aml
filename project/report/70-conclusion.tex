\section{Conclusion}
\label{sec:conclusion}

In this research we investigate the applicability of a recently appeared new implementation of GDBTs CatBoost with inbuilt intricate categorical feature preprocessing and advanced approach to fighting the bias in gradient estimation. We employ two more competitive GBDT implementations and compare their performance of three different datasets for housing price prediction. During the research, we found out that CatBoost outperforms two models only on the smallest dataset with the smallest number of features while still deferring to them in more realistic dataset. Further analysis shows its apparent preference towards the categorical variables while building the solution.

In case we would have more time and additional resources, we would firstly employ the GPU version of the CatBoost library which could gain additional score improvement taking into account long training time which prevented us from fully exploring all the powers of this library. Also it can be beneficial for all three libraries to explore smaller feature subsets as for close-to-life datasets they are really sparse which introduces much noise in the data and can be one of possible reasons of errors. Finally, it would be interesting to extend this research to other regression problems providing more general approach to this problem without focusing on specific things which are typical to house price prediction area.