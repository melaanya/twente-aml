\section{Discussion}
\label{sec:discussion}
It has been observed throughout the experiments that all three models show rather similar performance on the housing price datasets under consideration. In order to investigate the contribution of categorical variables into making the final decision we explore the feature importances provided by each of the GBDT implementations. Feature importance reflects the  informativeness of the characteristic for the model. It is computed as the normalized total reduction of the criterion brought by that feature and is also known as the Gini importance. We examine the number of categorical variables in the top 10 most important features for each model and summarize the observations in \cref{tab:feature-importance}.

\begin{table}[htbp]
	\centering
	\resizebox{0.7\linewidth}{!}{
		\begin{tabular}{lrrr}
			\toprule
			\textbf{Model} & \textbf{Russia} & \textbf{Ames} & \textbf{Boston} \\
			\midrule
			CatBoost & 5& 9 & 1 \\
			XGBoost & 1  & 1 & 0 \\
			sklearn & 1 & 0 & 0 \\
			\midrule
			\textbf{Number of categorical features} & 18 & 43 & 1 \\
			\textbf{Total number of features} & 293 & 80 & 13 \\
			\bottomrule
		\end{tabular}
	}
	\caption{Number of categorical features among top 10 most important for the model }
	\label{tab:feature-importance}
\end{table}

As can be seen from \cref{tab:feature-importance}, CatBoost gives clear precedence to the categorical variables whereas other models do not demonstrate any particular preference for them. We assume that this is due to the sophisticated encoding of categorical variables provided by the CatBoost. However, this special treatment does not necessarily boost the performance whereas seriously increasing the training time as we could perceive from \cref{sec:results}. 

With respect to the encountered experience of applying CatBoost to the housing price prediction task, we see that it obtains sufficient results when compared to other state-of-the-art models. However, it does not consistently outperform them and one possible reason for this can be its still imperfect optimization for regression problems which would also explain the absence of information regarding its performance on regression tasks. Another prominent problem that we came across is long training time of this model which prevented us from performing more extensive grid search. It is conceivable that with more training resources we could have gained better score for CatBoost as apparently, it incorporates elaborate approaches towards categorical variable processing and building weak estimators which require much more computational power that we has at our disposal. However, it is an open question whether several percent improvements are worth significant training time consuming.