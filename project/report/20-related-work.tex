\section{Related Work}
\label{sec:related-work}

To the best of our knowledge, the paper written by \citet{breiman1997arcing}  is one of the earliest in the GBDT field. It comprises the idea of viewing gradient boosting as the optimization algorithm minimizing certain cost function. This concept was subsequently developed by \citet{friedman2001greedy} who proposed to make the connection between stagewise model expansions and steepest-descent minimization and to consider boosting algorithms as iterative functional gradient descent algorithms. The intuition behind that work is that function minimization can be performed by repeatedly adding an imperfect model pointing to the direction of negative gradient to the existing one so as to correct the errors of the predecessors.

The gradient boosting is frequently used with decision trees as base functions. To determine the next base predictor, the enumeration of all possible splits on all the features has to be performed. The GBDT implementation provided by scikit-learn \cite{scikit-learn} employs exact greedy algorithm for choosing the predictor contributing the greatest score improvement. Since complete enumeration is computationally demanding, \citet{chen2016xgboost} propose an implementation called XGBoost which comprises an approximate algorithm for enumerating all possible splits in continuous features and several system design optimizations. These improvements allow to create a well-scalable model which outperforms the models existing by that time together with significant training process speed-up.

One of the most recent studies by \citet{DBLP:journals/corr/DorogushGGKPV17}, \cite{dorogushcatboost} proposes a new gradient-boosting library CatBoost which incorporates information contained in categorical features establishing a new strategy of computing the statistics for the category labels. Another crucial enhancement is a new way of 
identifying and solving a bias problem in pointwise gradient estimates which is present in classical boosting algorithms and their existing implementations. Proposed implementation outperforms the existing libraries in terms of prediction quality on several benchmarks in classification tasks without any regards to regression problems which served us a motivation to perform a research in this particular area.