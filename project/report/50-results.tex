\section{Results}
\label{sec:results}
This section discusses the experimental results of this project. We present the RMSLE scores for both default and tuned models when cross-validated on each of three datasets together with the training time taken.  

\subsection{Sberbank Dataset}
From our perspective, this dataset can be called the most challenging among the observed ones but at the same time the most practice-oriented as it comprises substantial amount of information incorporated in 293 features and more than 12000 features. As can be seen from \cref{tab:sberbank-results}, the performance of all three models is very similar for both default and tuned models, although XGBoost slightly outperforms the others. However, they significantly differ in the terms of training time: CatBoost is more than four times slower than two other models. 

\begin{table}[htbp]
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lrrrr}
			\toprule
			\textbf{Model} & \textbf{Default} & \textbf{Tuned} & \textbf{Max\_depth, n\_estimators, learning rate} & \textbf{Training time} \\
			\midrule
			CatBoost & 0.5161 &  0.5102 & 3, 100, 0.01 & 765.91 \\
			XGBoost & \textbf{0.5098} & \textbf{0.5081} & 2, 500, 0.05 & 158.67 \\
			sklearn GB & 0.5102 & 0.5091 & 1, 1000, 0.1  & \textbf{148.41} \\
			\bottomrule
		\end{tabular}
	}
	\caption{Sberbank dataset: RMSLE,  best parameters for tuned model, training time. The best result in each column is highlighted in bold.}
	\label{tab:sberbank-results}
\end{table}

\subsection{Ames Dataset}
\cref{tab:ames-results} presents the results obtained when training the models on Ames dataset. Examining it, we can derive that sklearn GB and XGBoost do comparably well with CatBoost slightly lagging behind for both the default and tuned models. We expected CatBoost to outperform the other models on this dataset as more than a half of its features are categorical. However, special processing of categorical features does not result in the substantial performance gain.

\begin{table}[htbp]
	\centering
	\begin{tabular}{lrrrr}
		\toprule
		\textbf{Model} & \textbf{Default} & \textbf{Tuned} & \textbf{Max\_depth, n\_estimators, learning rate} & \textbf{Training time}  \\
		\midrule
		CatBoost & 0.1789 &  0.1435 & 1, 1000, 0.1  & 58.82\\
		XGBoost & 0.1315 & 0.1246 & 2, 500, 0.1 & \textbf{8.72} \\
		sklearn GB & \textbf{0.1285} & \textbf{0.1241} & 1, 1000, 0.1 & 10.10 \\
		\bottomrule
	\end{tabular}
	\caption{Ames dataset: RMSLE, best parameters for tuned model, training time. The best result in each column is highlighted in bold.}
	\label{tab:ames-results}
\end{table}

\subsection{Boston Dataset}
It can be observed from the results for Boston dataset given in \cref{tab:boston-results} that it is the only dataset on which CatBoost succeeds to outperform other GDBT implemenations. However, this slight advantage gained in performance comes at the cost of training time taken: XGBoost is approximately 75 times faster than CatBoost and sklearn GB is 52 times faster than CatBoost. 

\begin{table}[htbp]
	\centering
	\begin{tabular}{lrrrr}
		\toprule
		\textbf{Model} & \textbf{Default} & \textbf{Tuned} & \textbf{Max\_depth, n\_estimators, learning rate} & \textbf{Training time}  \\
		\midrule
		CatBoost & \textbf{0.1584} &  \textbf{0.1423} & 1, 1000, 0.01  & 56.33 \\
		XGBoost & 0.1952 & 0.1753  & 2, 1000, 0.01  & \textbf{0.75} \\
		sklearn GB & 0.1993 & 0.1721 & 2, 1000, 0.01  & 1.07 \\
		\bottomrule
	\end{tabular}
    \caption{Boston dataset: RMSLE, best parameters for tuned model, training time. The best result in each column is highlighted in bold.}
	\label{tab:boston-results}
\end{table}



