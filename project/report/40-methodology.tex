\section{Methodology}
\label{sec:methodology}

This section briefly explains the main differences of CatBoost library from the wide-spread industrial gradient boosting implementations such as scikit-learn and XGBoost. Furthermore, we discuss performed data preprocessing and the evaluating procedure.

\subsection{Model}

In our research we compare the performance of CatBoostRegressor from CatBoost library with other two existing publicly available implementations of gradient boosting algorithm for regression problems: GradientBoostingRegressor from scikit-learn and XGBoostRegressor from XGBoost. There are two fundamental key points which we consider remarkable in the CatBoost implementation.

Firstly, it adopts an efficient strategy for handling categorical features in the data which is essentially substituting the category labels with some statistics computed per category with incorporated intention of prevented overfitting. To accomplish this, it performs a random permutation of the dataset and for each example it computes average label value for the example with the same category value placed before the given one in the permutation. 
If we denote the permutation as $ \sigma = (\sigma_1, ..., \sigma_n)$, the feature vector as $ X_i = (x_{i, 1}, ..., x_{i, n})$, the label value as $Y_i$ and the prior value with its weight as $P$ and $a$ respectively then the statistics is computed by the following formula:

$$ \frac{\sum_{j=1}^{p-1} [ x_{\sigma_j, i} = x_{\sigma_p, i}] Y_{\sigma_j} + a \cdot P}{\sum_{j=1}^{p-1} [ x_{\sigma_j, i} = x_{\sigma_p, i}] + a }. $$

Adding the prior helps reducing the noise obtained from low-frequency categories. The standard technique for choosing the prior in regression problems is to take the average label value in the dataset.

Secondly, CatBoost suggests a principled way of mitigating the problem of biased pointwise gradient estimates and proposes dynamic boosting approach that avoids the estimation bias at a minimal cost of the variance of the gradient estimation. Usually gradients on each iteration are assessed using the same data points which leads to a shift from actual distribution of gradients in any domain in the feature space. To overcome this problem, for each training instance a separate model is employed and it is never updated using the gradient estimate for this example. CatBoost implementation follows one relaxation of this idea which makes it feasible to employ: all these separate models share the same tree structures.

\subsection{Training}

In this project we examine the performance of three classifiers, namely GradientBoostingRegressor from scikit-learn, XGBoostRegressor from XGBoost and CatBoostRegressor from CatBoost. We assess the performance of these models in the field of housing price prediction by applying them on three different datasets without any preliminary feature selection as we do not opt for gaining the highest but for analyzing models conduct in the same conditions.

In order to make use of GradientBoostingRegressor we first need to preprocess the data: for numerical columns we replace the missing values with the mean along each column whereas for categorical variables we perform one-hot encoding. Regarding XGBoostRegressor, we solely rely on its internal missing values treatment but repeat the procedure of binarizing the categorical variables. We do not adopt any additional feature preprocessing for CatBoostRegressor, only explicitly specify the categorical variables for its further internal processing. All the models are first trained with their default parameters which one can find in Appendix \cref{tab:default-parameters} and later tuned by employing the grid search over the predefined set of parameters which can be found in Appendix \cref{tab:grid-search-parameters}.

\subsection{Evaluation}

To evaluate the regression problem we employ the Root Mean Squared Logarithmic Error (RMSLE) as it equally penalizes the mismatches within small and huge values. Let $p_i$ be the predicted values and $a_i$ the actual values, then RMSLE is computed as follows:

$$ RMSLE = \sqrt[]{\frac{1}{N}\sum_{n = 1}^N (\log(p_i + 1) - \log(a_i + 1))^2}. $$

In order to assess the performance of the discussed algorithms and perform the grid search, we employ 5-fold cross validation. Throughout this research, the training speed is also of great interest for us, and thus, we measure the training time 3 times in a row and report the average of them.



